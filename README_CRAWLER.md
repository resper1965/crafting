# Crawler - Crafting Solutions

## ğŸ“‹ VisÃ£o Geral

Crawler Node.js para extrair conteÃºdo do site antigo `https://projetos.tangomarketing.com.br/craftingsolutions/` e gerar dataset para refatoraÃ§Ã£o.

## ğŸš€ Uso RÃ¡pido

```bash
cd crawler
npm install
node crawl.js --out ./out
```

## âš™ï¸ OpÃ§Ãµes

```bash
node crawl.js [opÃ§Ãµes]

OpÃ§Ãµes:
  --out <diretÃ³rio>        DiretÃ³rio de saÃ­da (padrÃ£o: ./out)
  --max-urls <nÃºmero>      MÃ¡ximo de URLs a processar (padrÃ£o: 5000)
  --max-depth <nÃºmero>      Profundidade mÃ¡xima do crawl (padrÃ£o: 6)
  --concurrency <nÃºmero>   RequisiÃ§Ãµes simultÃ¢neas (padrÃ£o: 5)
  --dry-run                Modo teste (nÃ£o faz requisiÃ§Ãµes reais)
```

## ğŸ“Š SaÃ­das

O crawler gera 3 arquivos:

1. **`out/urls.txt`** - Lista de URLs encontradas (1 por linha)
2. **`out/urls.csv`** - CSV com metadados (url, status_code, content_type, depth, title, h1, word_count)
3. **`out/pages.json`** - Dataset completo com conteÃºdo extraÃ­do

## ğŸ”’ RestriÃ§Ãµes

- âœ… Aceita **SOMENTE** URLs que comeÃ§am com `https://projetos.tangomarketing.com.br/craftingsolutions/`
- âœ… Ignora links externos e assets (jpg, png, pdf, css, js, etc)
- âœ… Remove parÃ¢metros de tracking (utm_*, fbclid, gclid)
- âœ… Normaliza URLs (remove fragmentos, trailing slashes)
- âœ… Respeita robots.txt

## ğŸ“ Estrutura do pages.json

```json
[
  {
    "url": "https://...",
    "statusCode": 200,
    "contentType": "text/html; charset=UTF-8",
    "depth": 0,
    "discoveredFrom": null,
    "title": "TÃ­tulo da pÃ¡gina",
    "headings": {
      "h1": ["TÃ­tulo H1"],
      "h2": ["SubtÃ­tulo 1", "SubtÃ­tulo 2"],
      "h3": []
    },
    "text": "ConteÃºdo principal extraÃ­do...",
    "wordCount": 123,
    "extractedAt": "2026-01-12T20:43:37.287Z"
  }
]
```

## ğŸ”§ Desenvolvimento

### Estrutura do CÃ³digo

```
crawler/
â”œâ”€â”€ crawl.js           # CLI principal
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ url-utils.js   # NormalizaÃ§Ã£o e validaÃ§Ã£o de URLs
â”‚   â”œâ”€â”€ fetcher.js     # HTTP client com retry/backoff
â”‚   â”œâ”€â”€ parser.js      # ExtraÃ§Ã£o de conteÃºdo HTML
â”‚   â”œâ”€â”€ sitemap.js     # Descoberta e parsing de sitemaps
â”‚   â””â”€â”€ crawler.js     # LÃ³gica de crawl BFS
â””â”€â”€ out/               # SaÃ­das (ignorado no git)
```

### DependÃªncias

- `cheerio` - Parser HTML
- `fast-xml-parser` - Parser de sitemaps XML
- `p-limit` - Controle de concorrÃªncia

## ğŸ“š DocumentaÃ§Ã£o Relacionada

- `docs/DESIGN_SPEC.md` - Design spec baseado em conteÃºdo extraÃ­do
- `docs/CRAWLER_REFATORACAO_RESUMO.md` - Resumo completo da implementaÃ§Ã£o
- `PLANO_EXECUCAO.md` - Plano de execuÃ§Ã£o original

## âš ï¸ Notas

- O crawler usa `fetch` nativo do Node.js 18+
- Timeout padrÃ£o: 15 segundos
- Retry: 2 tentativas com backoff exponencial
- ConcorrÃªncia padrÃ£o: 5 requisiÃ§Ãµes simultÃ¢neas
